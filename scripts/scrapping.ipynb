{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8405c1a1-0992-4ead-97e3-519ab5107e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement opeanai (from versions: none)\n",
      "ERROR: No matching distribution found for opeanai\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager\n",
    "!pip install bs4\n",
    "!pip install openpyxl\n",
    "!pip install opeanai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ef1136b-2c9c-43f9-8ccf-525a1e3ef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuración\n",
    "DIRECTORIO = \"noticias_desmog\"\n",
    "ARCHIVO_CONSOLIDADO = os.path.join(DIRECTORIO, 'desmog_news.txt')\n",
    "SEPARADOR = \"\\n\" + \"-\" * 80 + \"\\n\"\n",
    "BASE_URL = \"https://www.desmog.com/sitesearch/?q={query}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(DIRECTORIO, exist_ok=True)\n",
    "\n",
    "# Configurar Selenium con Chrome Headless\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6f335a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_popups():\n",
    "    \"\"\"Cierra pop-ups de suscripción y cookies si aparecen.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Accept')]\"))\n",
    "        ).click()\n",
    "        print(\"Pop-up de cookies cerrado.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No se encontró pop-up de cookies.\")\n",
    "    try:\n",
    "        close_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@aria-label, 'Close')]\")\n",
    "        ))\n",
    "        close_button.click()\n",
    "        print(\"Pop-up de suscripción cerrado.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No se encontró el botón de cierre del pop-up. Intentando cerrar con ESCAPE...\")\n",
    "        try:\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)\n",
    "            print(\"Pop-up cerrado con tecla ESCAPE.\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"No se pudo cerrar el pop-up de suscripción.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "053f6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_and_load_results():\n",
    "    \"\"\"Desplaza la página para cargar más resultados.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "760cc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    cursor_box = soup.find('div', class_='gsc-cursor-box gs-bidi-start-align')\n",
    "    if cursor_box is not None:\n",
    "        length = len(cursor_box.find_all()) - 1\n",
    "    else:\n",
    "        length = 0\n",
    "    if cursor_box:\n",
    "        # Encuentra el número de la página actual\n",
    "        current_page = cursor_box.find('div', class_='gsc-cursor-current-page')\n",
    "        if current_page:\n",
    "            current_page_number = int(current_page.get_text(strip=True))\n",
    "            next_page_number = current_page_number + 1\n",
    "            \n",
    "            # Limitar a 10 páginas\n",
    "            print(f'longitud: {length}\\n')\n",
    "            if next_page_number > min(length, 10):\n",
    "                return False\n",
    "            \n",
    "            # Encuentra el botón de la siguiente página\n",
    "            next_page_div = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//div[@class='gsc-cursor-page' and text()='{next_page_number}']\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page_div)\n",
    "            time.sleep(2)  # Espera a que cargue la siguiente página\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_page(keyword):\n",
    "    time.sleep(1)\n",
    "    scroll_and_load_results()\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gsc-webResult') or soup.find_all('div', class_='gsc-result')\n",
    "    \n",
    "    noticias = []\n",
    "    for article in articles:\n",
    "        title_tag = article.find('a', class_='gs-title')\n",
    "        if title_tag and title_tag.get('href'):\n",
    "            title = title_tag.text.strip()\n",
    "            link = title_tag['href']\n",
    "            noticias.append({\"title\": title, \"link\": link, \"keyword\": keyword})\n",
    "    return noticias\n",
    "\n",
    "def obtener_resultados_busqueda(query):\n",
    "    \"\"\"Obtiene los enlaces y títulos de los resultados de búsqueda, manejando ventanas emergentes si aparecen.\"\"\"\n",
    "    url = BASE_URL.format(query=query)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_popups()\n",
    "    noticias = scan_page(query)\n",
    "    i=1\n",
    "    # print('EXPLORACIÓN')\n",
    "    # while(next_page()):\n",
    "    #     print(f'Explorando página {i} de resultados')\n",
    "    #     noticias = noticias + scan_page(query)  \n",
    "    #     i += 1\n",
    "    #     break\n",
    "        \n",
    "    return noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b94bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_contenido_noticia(url, keyword):\n",
    "    \"\"\"Extrae el contenido, autor, fecha y título de una noticia.\"\"\"\n",
    "    driver.get(url) \n",
    "    time.sleep(2.5)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "    )\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extraer título\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"Título no disponible\"\n",
    "    \n",
    "    # Extraer autor\n",
    "    # author_tag = soup.find('span', class_='author-name')\n",
    "    # author_tag = soup.find(class_='jet-listing-dynamic-meta__item-val molongui-disabled-link')\n",
    "    author_tag = soup.select(\"div.jet-listing-dynamic-meta__author a\")\n",
    "    # print(f'el autor está en {author_tag}')\n",
    "    author = author_tag[0].get_text(strip=True) if author_tag else \"Autor no disponible\"\n",
    "    \n",
    "    # Extraer fecha\n",
    "    date_tag = soup.find('time')\n",
    "    date = date_tag.get_text(strip=True) if date_tag else \"Fecha no disponible\"\n",
    "    \n",
    "    # Extraer contenido\n",
    "    paragraphs = soup.find_all(['p', 'h2', 'h3'])\n",
    "    content = '\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "    content = content if content else \"Contenido no disponible.\"\n",
    "    \n",
    "    # return f\"Título: {title}\\nAutor: {author}\\nFecha: {date}\\n\\n{content}\"\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'content': content,\n",
    "        'url': url,\n",
    "        'keyword': keyword\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bad172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# def guardar_noticias(noticias):\n",
    "#     with open('resultados.csv', 'w', encoding=\"utf-8\") as file:\n",
    "#         write = csv.writer(file)\n",
    "#         # write.writerows(csv_list)\n",
    "#         write.writerow(['Título', 'Autor', 'Fecha', 'Contenido'])\n",
    "        \n",
    "        \n",
    "#         # resultado = []\n",
    "#         print('PROCESAMIENTO')\n",
    "#         for idx, noticia in enumerate(noticias):\n",
    "#                 print(f'Procesando noticia {idx}/{len(noticias)}')\n",
    "#                 write.writerow(list(obtener_contenido_noticia(noticia['link']).values()))\n",
    "\n",
    "#         # csv_list_header = list(noticias[0].values())\n",
    "#         # csv_list_body = [list(noticia.values()) in noticias]\n",
    "#         # csv_list = csv_list_header + csv_list_body\n",
    "#     print(f\"Noticias guardadas en resultados.csv\")\n",
    "          \n",
    "# #     with open('noticias.json', 'w', encoding='utf-8') as json_file:\n",
    "# #             # f.write(f\"{SEPARADOR}{contenido}{SEPARADOR}\")\n",
    "# #             json.dump(resultado,json_file)\n",
    "\n",
    "def guardar_noticias(noticias, docTitle, keyword):\n",
    "    datos = []\n",
    "    print(\"PROCESAMIENTO\")\n",
    "    \n",
    "    for idx, noticia in enumerate(noticias):\n",
    "        print(f'Procesando noticia {idx + 1}/{len(noticias)}')\n",
    "        contenido = obtener_contenido_noticia(noticia['link'], noticia['keyword'])\n",
    "        datos.append(contenido)  # Agregamos el contenido al DataFrame\n",
    "\n",
    "    # Convertimos la lista de datos en un DataFrame\n",
    "    df = pd.DataFrame(datos)\n",
    "    \n",
    "    # Guardamos el DataFrame en un archivo Excel\n",
    "    file_name = f'scrap_{docTitle}.xlsx'\n",
    "    df.to_excel(file_name, index=False)\n",
    "    print(f\"Noticias guardadas en {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2667f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_keywords2 = [\"Eco-anxiety\", \"papucho\", \"Tipping point\"]\n",
    "climate_keywords = [\n",
    "    \"Regenerative agriculture\",\n",
    "    \"Eco-anxiety\",\n",
    "    \"Tipping point\",\n",
    "    \"Mitigation\",\n",
    "    \"Adaptation\",\n",
    "    \"Resilience\",\n",
    "    \"Carbon footprint\",\n",
    "    \"Water footprint\",\n",
    "    \"Climate justice\",\n",
    "    \"Nature-based solutions (NbS)\",\n",
    "    \"Loss and damage\",\n",
    "    \"Net zero emissions\",\n",
    "    \"Decarbonization\",\n",
    "    \"Carbon sink\",\n",
    "    \"Carbon sequestration\",\n",
    "    \"Rewilding\",\n",
    "    \"Circular economy\",\n",
    "    \"Greenwashing\",\n",
    "    \"Nature crisis\",\n",
    "    \"Biodiversity crisis\",\n",
    "    \"Planetary boundaries\",\n",
    "    \"Land degradation\",\n",
    "    \"Biodiversity hotspot\",\n",
    "    \"Environmental justice\",\n",
    "    \"Ecocide\",\n",
    "    \"Indigenous knowledge\",\n",
    "    \"Traditional ecological knowledge\",\n",
    "    \"Green economy\",\n",
    "    \"Blue economy\",\n",
    "    \"Reforestation and afforestation\",\n",
    "    \"Debt-for-nature swaps\",\n",
    "    \"Access and benefit sharing (ABS)\",\n",
    "    \"Green jobs\",\n",
    "    \"Global Biodiversity Framework (GBF)\",\n",
    "    \"National Biodiversity Strategies and Action Plans (NBSAPs)\",\n",
    "    \"Nature-positive\",\n",
    "    \"Weather vs climate\",\n",
    "    \"Greenhouse gases (GHGs)\",\n",
    "    \"Global warming\",\n",
    "    \"Climate change\",\n",
    "    \"Climate crisis\",\n",
    "    \"Feedback loops\",\n",
    "    \"Overshoot\",\n",
    "    \"Climate security\",\n",
    "    \"Climate finance\",\n",
    "    \"Renewable energy\",\n",
    "    \"Carbon removal vs carbon sequestration\",\n",
    "    \"Carbon markets\",\n",
    "    \"Just transition\",\n",
    "    \"United Nations Framework Convention on Climate Change (UNFCCC)\",\n",
    "    \"Conference of the Parties (COP)\",\n",
    "    \"Paris Agreement\",\n",
    "    \"Nationally Determined Contributions (NDCs)\",\n",
    "    \"Transparency\",\n",
    "    \"National Adaptation Plans (NAPs)\",\n",
    "    \"Long-term strategies (LTS)\",\n",
    "    \"REDD+\",\n",
    "    \"Intergovernmental Panel on Climate Change (IPCC)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1b25c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando noticias para: Regenerative agriculture 0/58\n",
      "Pop-up de cookies cerrado.\n",
      "No se encontró el botón de cierre del pop-up. Intentando cerrar con ESCAPE...\n",
      "Pop-up cerrado con tecla ESCAPE.\n",
      "EXPLORACIÓN\n",
      "longitud: 6\n",
      "\n",
      "Explorando página 1 de resultados\n",
      "Buscando noticias para: Eco-anxiety 1/58\n",
      "No se encontró pop-up de cookies.\n",
      "No se encontró el botón de cierre del pop-up. Intentando cerrar con ESCAPE...\n",
      "Pop-up cerrado con tecla ESCAPE.\n",
      "EXPLORACIÓN\n",
      "longitud: 3\n",
      "\n",
      "Explorando página 1 de resultados\n",
      "Buscando noticias para: Tipping point 2/58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, keyword \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(climate_keywords):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuscando noticias para: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(climate_keywords)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     noticias \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mobtener_resultados_busqueda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noticias:\n\u001b[0;32m      9\u001b[0m     guardar_noticias(noticias, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiccionario\u001b[39m\u001b[38;5;124m'\u001b[39m, keyword)\n",
      "Cell \u001b[1;32mIn[48], line 20\u001b[0m, in \u001b[0;36mobtener_resultados_busqueda\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     18\u001b[0m url \u001b[38;5;241m=\u001b[39m BASE_URL\u001b[38;5;241m.\u001b[39mformat(query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m     19\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m close_popups()\n\u001b[0;32m     22\u001b[0m noticias \u001b[38;5;241m=\u001b[39m scan_page(query)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    # query = input(\"Ingrese la palabra clave para buscar en Desmog: \")\n",
    "    noticias = []\n",
    "    for idx, keyword in enumerate(climate_keywords):\n",
    "        print(f\"Buscando noticias para: {keyword} {idx}/{len(climate_keywords)}\")\n",
    "        noticias += obtener_resultados_busqueda(keyword)\n",
    "    if noticias:\n",
    "        guardar_noticias(noticias, 'diccionario', keyword)\n",
    "        # guardar_noticias([noticias[0]])\n",
    "    else:\n",
    "        print(\"No se encontraron noticias.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28108bdf",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
