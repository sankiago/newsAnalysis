{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405c1a1-0992-4ead-97e3-519ab5107e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\lpzsd\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\lpzsd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager\n",
    "!pip install bs4\n",
    "!pip install openpyxl\n",
    "!pip install opeanai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef1136b-2c9c-43f9-8ccf-525a1e3ef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuración\n",
    "DIRECTORIO = \"noticias_desmog\"\n",
    "ARCHIVO_CONSOLIDADO = os.path.join(DIRECTORIO, 'desmog_news.txt')\n",
    "SEPARADOR = \"\\n\" + \"-\" * 80 + \"\\n\"\n",
    "BASE_URL = \"https://www.desmog.com/sitesearch/?q={query}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(DIRECTORIO, exist_ok=True)\n",
    "\n",
    "# Configurar Selenium con Chrome Headless\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f335a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_popups():\n",
    "    \"\"\"Cierra pop-ups de suscripción y cookies si aparecen.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Accept')]\"))\n",
    "        ).click()\n",
    "        print(\"Pop-up de cookies cerrado.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No se encontró pop-up de cookies.\")\n",
    "    try:\n",
    "        close_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@aria-label, 'Close')]\")\n",
    "        ))\n",
    "        close_button.click()\n",
    "        print(\"Pop-up de suscripción cerrado.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No se encontró el botón de cierre del pop-up. Intentando cerrar con ESCAPE...\")\n",
    "        try:\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)\n",
    "            print(\"Pop-up cerrado con tecla ESCAPE.\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"No se pudo cerrar el pop-up de suscripción.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053f6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_and_load_results():\n",
    "    \"\"\"Desplaza la página para cargar más resultados.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760cc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    cursor_box = soup.find('div', class_='gsc-cursor-box gs-bidi-start-align')\n",
    "    \n",
    "    if cursor_box:\n",
    "        # Encuentra el número de la página actual\n",
    "        current_page = cursor_box.find('div', class_='gsc-cursor-current-page')\n",
    "        if current_page:\n",
    "            current_page_number = int(current_page.get_text(strip=True))\n",
    "            next_page_number = current_page_number + 1\n",
    "            \n",
    "            # Limitar a 10 páginas\n",
    "            if next_page_number > 10:\n",
    "                return False\n",
    "            \n",
    "            # Encuentra el botón de la siguiente página\n",
    "            next_page_div = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, f\"//div[@class='gsc-cursor-page' and text()='{next_page_number}']\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page_div)\n",
    "            time.sleep(2)  # Espera a que cargue la siguiente página\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff09d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_page():\n",
    "    time.sleep(1)\n",
    "    scroll_and_load_results()\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gsc-webResult') or soup.find_all('div', class_='gsc-result')\n",
    "    \n",
    "    noticias = []\n",
    "    for article in articles:\n",
    "        title_tag = article.find('a', class_='gs-title')\n",
    "        if title_tag and title_tag.get('href'):\n",
    "            title = title_tag.text.strip()\n",
    "            link = title_tag['href']\n",
    "            noticias.append({\"title\": title, \"link\": link})\n",
    "    return noticias\n",
    "\n",
    "def obtener_resultados_busqueda(query):\n",
    "    \"\"\"Obtiene los enlaces y títulos de los resultados de búsqueda, manejando ventanas emergentes si aparecen.\"\"\"\n",
    "    url = BASE_URL.format(query=query)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_popups()\n",
    "    noticias = scan_page()\n",
    "    i=1\n",
    "    print('EXPLORACIÓN')\n",
    "    while(next_page()):\n",
    "        print(f'Explorando página {i} de resultados')\n",
    "        noticias = noticias + scan_page()  \n",
    "        i += 1\n",
    "        \n",
    "    return noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b94bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_contenido_noticia(url):\n",
    "    \"\"\"Extrae el contenido, autor, fecha y título de una noticia.\"\"\"\n",
    "    driver.get(url) \n",
    "    time.sleep(2.5)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "    )\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extraer título\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"Título no disponible\"\n",
    "    \n",
    "    # Extraer autor\n",
    "    # author_tag = soup.find('span', class_='author-name')\n",
    "    # author_tag = soup.find(class_='jet-listing-dynamic-meta__item-val molongui-disabled-link')\n",
    "    author_tag = soup.select(\"div.jet-listing-dynamic-meta__author a\")\n",
    "    # print(f'el autor está en {author_tag}')\n",
    "    author = author_tag[0].get_text(strip=True) if author_tag else \"Autor no disponible\"\n",
    "    \n",
    "    # Extraer fecha\n",
    "    date_tag = soup.find('time')\n",
    "    date = date_tag.get_text(strip=True) if date_tag else \"Fecha no disponible\"\n",
    "    \n",
    "    # Extraer contenido\n",
    "    paragraphs = soup.find_all(['p', 'h2', 'h3'])\n",
    "    content = '\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "    content = content if content else \"Contenido no disponible.\"\n",
    "    \n",
    "    # return f\"Título: {title}\\nAutor: {author}\\nFecha: {date}\\n\\n{content}\"\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'content': content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# def guardar_noticias(noticias):\n",
    "#     with open('resultados.csv', 'w', encoding=\"utf-8\") as file:\n",
    "#         write = csv.writer(file)\n",
    "#         # write.writerows(csv_list)\n",
    "#         write.writerow(['Título', 'Autor', 'Fecha', 'Contenido'])\n",
    "        \n",
    "        \n",
    "#         # resultado = []\n",
    "#         print('PROCESAMIENTO')\n",
    "#         for idx, noticia in enumerate(noticias):\n",
    "#                 print(f'Procesando noticia {idx}/{len(noticias)}')\n",
    "#                 write.writerow(list(obtener_contenido_noticia(noticia['link']).values()))\n",
    "\n",
    "#         # csv_list_header = list(noticias[0].values())\n",
    "#         # csv_list_body = [list(noticia.values()) in noticias]\n",
    "#         # csv_list = csv_list_header + csv_list_body\n",
    "#     print(f\"Noticias guardadas en resultados.csv\")\n",
    "          \n",
    "# #     with open('noticias.json', 'w', encoding='utf-8') as json_file:\n",
    "# #             # f.write(f\"{SEPARADOR}{contenido}{SEPARADOR}\")\n",
    "# #             json.dump(resultado,json_file)\n",
    "\n",
    "def guardar_noticias(noticias, query):\n",
    "    datos = []\n",
    "    print(\"PROCESAMIENTO\")\n",
    "    \n",
    "    for idx, noticia in enumerate(noticias):\n",
    "        print(f'Procesando noticia {idx + 1}/{len(noticias)}')\n",
    "        contenido = obtener_contenido_noticia(noticia['link'])\n",
    "        datos.append(contenido)  # Agregamos el contenido al DataFrame\n",
    "\n",
    "    # Convertimos la lista de datos en un DataFrame\n",
    "    df = pd.DataFrame(datos)\n",
    "    \n",
    "    # Guardamos el DataFrame en un archivo Excel\n",
    "    file_name = f'resultados_{query}.xlsx'\n",
    "    df.to_excel(file_name, index=False)\n",
    "    print(f\"Noticias guardadas en {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b25c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando noticias para: pipeline\n",
      "Pop-up de cookies cerrado.\n",
      "No se encontró el botón de cierre del pop-up. Intentando cerrar con ESCAPE...\n",
      "Pop-up cerrado con tecla ESCAPE.\n",
      "EXPLORACIÓN\n",
      "Explorando página 1 de resultados\n",
      "Explorando página 2 de resultados\n",
      "Explorando página 3 de resultados\n",
      "Explorando página 4 de resultados\n",
      "Explorando página 5 de resultados\n",
      "Explorando página 6 de resultados\n",
      "Explorando página 7 de resultados\n",
      "Explorando página 8 de resultados\n",
      "Explorando página 9 de resultados\n",
      "PROCESAMIENTO\n",
      "Procesando noticia 1/110\n",
      "Procesando noticia 2/110\n",
      "Procesando noticia 3/110\n",
      "Procesando noticia 4/110\n",
      "Procesando noticia 5/110\n",
      "Procesando noticia 6/110\n",
      "Procesando noticia 7/110\n",
      "Procesando noticia 8/110\n",
      "Procesando noticia 9/110\n",
      "Procesando noticia 10/110\n",
      "Procesando noticia 11/110\n",
      "Procesando noticia 12/110\n",
      "Procesando noticia 13/110\n",
      "Procesando noticia 14/110\n",
      "Procesando noticia 15/110\n",
      "Procesando noticia 16/110\n",
      "Procesando noticia 17/110\n",
      "Procesando noticia 18/110\n",
      "Procesando noticia 19/110\n",
      "Procesando noticia 20/110\n",
      "Procesando noticia 21/110\n",
      "Procesando noticia 22/110\n",
      "Procesando noticia 23/110\n",
      "Procesando noticia 24/110\n",
      "Procesando noticia 25/110\n",
      "Procesando noticia 26/110\n",
      "Procesando noticia 27/110\n",
      "Procesando noticia 28/110\n",
      "Procesando noticia 29/110\n",
      "Procesando noticia 30/110\n",
      "Procesando noticia 31/110\n",
      "Procesando noticia 32/110\n",
      "Procesando noticia 33/110\n",
      "Procesando noticia 34/110\n",
      "Procesando noticia 35/110\n",
      "Procesando noticia 36/110\n",
      "Procesando noticia 37/110\n",
      "Procesando noticia 38/110\n",
      "Procesando noticia 39/110\n",
      "Procesando noticia 40/110\n",
      "Procesando noticia 41/110\n",
      "Procesando noticia 42/110\n",
      "Procesando noticia 43/110\n",
      "Procesando noticia 44/110\n",
      "Procesando noticia 45/110\n",
      "Procesando noticia 46/110\n",
      "Procesando noticia 47/110\n",
      "Procesando noticia 48/110\n",
      "Procesando noticia 49/110\n",
      "Procesando noticia 50/110\n",
      "Procesando noticia 51/110\n",
      "Procesando noticia 52/110\n",
      "Procesando noticia 53/110\n",
      "Procesando noticia 54/110\n",
      "Procesando noticia 55/110\n",
      "Procesando noticia 56/110\n",
      "Procesando noticia 57/110\n",
      "Procesando noticia 58/110\n",
      "Procesando noticia 59/110\n",
      "Procesando noticia 60/110\n",
      "Procesando noticia 61/110\n",
      "Procesando noticia 62/110\n",
      "Procesando noticia 63/110\n",
      "Procesando noticia 64/110\n",
      "Procesando noticia 65/110\n",
      "Procesando noticia 66/110\n",
      "Procesando noticia 67/110\n",
      "Procesando noticia 68/110\n",
      "Procesando noticia 69/110\n",
      "Procesando noticia 70/110\n",
      "Procesando noticia 71/110\n",
      "Procesando noticia 72/110\n",
      "Procesando noticia 73/110\n",
      "Procesando noticia 74/110\n",
      "Procesando noticia 75/110\n",
      "Procesando noticia 76/110\n",
      "Procesando noticia 77/110\n",
      "Procesando noticia 78/110\n",
      "Procesando noticia 79/110\n",
      "Procesando noticia 80/110\n",
      "Procesando noticia 81/110\n",
      "Procesando noticia 82/110\n",
      "Procesando noticia 83/110\n",
      "Procesando noticia 84/110\n",
      "Procesando noticia 85/110\n",
      "Procesando noticia 86/110\n",
      "Procesando noticia 87/110\n",
      "Procesando noticia 88/110\n",
      "Procesando noticia 89/110\n",
      "Procesando noticia 90/110\n",
      "Procesando noticia 91/110\n",
      "Procesando noticia 92/110\n",
      "Procesando noticia 93/110\n",
      "Procesando noticia 94/110\n",
      "Procesando noticia 95/110\n",
      "Procesando noticia 96/110\n",
      "Procesando noticia 97/110\n",
      "Procesando noticia 98/110\n",
      "Procesando noticia 99/110\n",
      "Procesando noticia 100/110\n",
      "Procesando noticia 101/110\n",
      "Procesando noticia 102/110\n",
      "Procesando noticia 103/110\n",
      "Procesando noticia 104/110\n",
      "Procesando noticia 105/110\n",
      "Procesando noticia 106/110\n",
      "Procesando noticia 107/110\n",
      "Procesando noticia 108/110\n",
      "Procesando noticia 109/110\n",
      "Procesando noticia 110/110\n",
      "Noticias guardadas en resultados.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    # query = input(\"Ingrese la palabra clave para buscar en Desmog: \")\n",
    "    query = 'pipeline'\n",
    "    print(f\"Buscando noticias para: {query}\")\n",
    "    noticias = obtener_resultados_busqueda(query)\n",
    "    if noticias:\n",
    "        guardar_noticias(noticias, query)\n",
    "        # guardar_noticias([noticias[0]])\n",
    "    else:\n",
    "        print(\"No se encontraron noticias.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28108bdf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336f284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
